[["index.html", "Post-Fire Conifer Regeneration Part 1 About", " Post-Fire Conifer Regeneration Casey Menick 2022-11-30 Part 1 About "],["fire-selection.html", "Part 2 Fire Selection 2.1 Set Up 2.2 Define Fire Parameters 2.3 Final Fire Dataset 2.4 Export Data", " Part 2 Fire Selection 2.1 Set Up 2.1.1 Libraries library(tidyverse) library(terra) library(sf) library(mapview) library(raster) library(rgeos) library(lubridate) library(ggplot2) library(exactextractr) library(patchwoRk) library(gridExtra) 2.1.2 USDA National Forest Type Group Dataset Conifer Forest Type Groups: Douglas-Fir, Fir-Spruce-Mountain Hemlock, Lodgepole Pine # forest type groups and key conus_forestgroup &lt;- raster(&#39;data/forest_type/conus_forestgroup.tif&#39;) forest_codes &lt;- read_csv(&#39;data/forest_type/forestgroupcodes.csv&#39;) # set crs crs = crs(conus_forestgroup) 2.1.3 EPA level-3 Ecoregions Canadian Rockies, Idaho Batholith, Middle Rockies, Columbian Mountains - Northern Rockies # level 3 ecoregions l3eco &lt;- st_read(&#39;data/ecoregion/us_eco_l3.shp&#39;) %&gt;% st_transform(., crs=crs) # select northern rocky mountains from level3 ecoregions eco_select &lt;- l3eco %&gt;% filter(NA_L3NAME %in% c(&#39;Canadian Rockies&#39;,&#39;Columbia Mountains/Northern Rockies&#39;,&#39;Middle Rockies&#39;,&#39;Idaho Batholith&#39;)) 2.1.4 Mapping 2.1.4.1 Ecoregions mapview(eco_select) 2.1.4.2 Forest Type Groups forestgroup_crop &lt;- crop(conus_forestgroup,l3eco) mapview(forestgroup_crop) 2.2 Define Fire Parameters 2.2.1 Monitoring Trends in Burn Severity (MTBS) Dataset Criteria: -1988-1991 -500+ acres of high-severity -Within selected ecoregions -&gt;25% of selected forest types # mtbs fire perimeters mtbs_full &lt;- st_read(&#39;data/mtbs/mtbs_perims_DD.shp&#39;) %&gt;% st_transform(., crs=crs) mtbs_select &lt;- mtbs_full %&gt;% mutate(state = str_sub(Event_ID,0,2), year = year(as.Date(Ig_Date))) %&gt;% filter(state %in% c(&quot;WA&quot;,&quot;ID&quot;,&quot;MT&quot;,&quot;WY&quot;,&quot;SD&quot;), between(Ig_Date, as.Date(&#39;1988-01-1&#39;), as.Date(&#39;1991-12-31&#39;))) 2.2.2 Group Adjacent Fires # function to group adjoining fire polygons to ensure contiguous high-severity patches group_fires &lt;- function(mtbs_year) { # join the polygons with themselves, and remove those that do not join with any besides themselves combined&lt;- st_join(mtbs_year, mtbs_year, join=st_is_within_distance, dist = 180, left = TRUE,remove_self = TRUE) %&gt;% drop_na(Event_ID.y)%&gt;% dplyr::select(Event_ID.x,Event_ID.y) if(nrow(combined)&gt;=1){ # if there are overlaps for this years fires... # partition data into that that has overlap, and that that does not overlap &lt;- mtbs_year %&gt;% filter(Event_ID %in% combined$Event_ID.x) no_overlap &lt;- mtbs_year %&gt;% filter(!(Event_ID %in% combined$Event_ID.x)) print(paste0(&quot;there are &quot;,nrow(overlap),&quot; overlapping polygons&quot;)) # join all overlapping features, and buffer to ensure proper grouping overlap_union &lt;- st_union(overlap) %&gt;% st_buffer(190) # break apart the joined polygons into their individual groups groups &lt;- st_as_sf(st_cast(overlap_union ,to=&#39;POLYGON&#39;,group_or_split=TRUE)) %&gt;% mutate(year = mean(mtbs_year$year), Fire_ID = str_c(&quot;Fire_&quot;,c(1:nrow(.)),&quot;_&quot;,year)) %&gt;% rename(geometry = x) print(paste0(&quot;polygons formed into &quot;,nrow(groups),&quot; groups&quot;)) # join back with original dataset to return to unbuffered geometry grouped_overlap &lt;- st_join(overlap,groups,left=TRUE) # arrange by the new grouping joined_overlap_groups &lt;- grouped_overlap %&gt;% group_by(Fire_ID) %&gt;% tally()%&gt;% st_buffer(1) %&gt;% dplyr::select(Fire_ID) %&gt;% mutate(year = mean(mtbs_year$year)) # add new ID to the freestanding polygons no_overlap_groups &lt;- no_overlap %&gt;% mutate(Fire_ID = str_c(&quot;Fire_&quot;,nrow(groups)+c(1:nrow(no_overlap)),&quot;_&quot;,year)) %&gt;% dplyr::select(Fire_ID,year) # join the new grouped overlap and the polygons without overlap fires_export &lt;- rbind(joined_overlap_groups,no_overlap_groups) return(fires_export) } else { # if there are no overlaps for this year... print(&quot;no overlapping polygons&quot;) fires_export &lt;- mtbs_year %&gt;% mutate(Fire_ID = str_c(&quot;Fire_&quot;,c(1:nrow(.)),&quot;_&quot;,year)) %&gt;% dplyr::select(Fire_ID,year) return(fires_export) } } # group adjacent polygons within each fire year fires_88 &lt;- group_fires(mtbs_select %&gt;% filter(year == 1988)) ## [1] &quot;there are 22 overlapping polygons&quot; ## [1] &quot;polygons formed into 7 groups&quot; fires_89 &lt;- group_fires(mtbs_select %&gt;% filter(year == 1989)) ## [1] &quot;there are 2 overlapping polygons&quot; ## [1] &quot;polygons formed into 1 groups&quot; fires_90 &lt;- group_fires(mtbs_select %&gt;% filter(year == 1990)) ## [1] &quot;there are 2 overlapping polygons&quot; ## [1] &quot;polygons formed into 1 groups&quot; fires_91 &lt;- group_fires(mtbs_select %&gt;% filter(year == 1991)) ## [1] &quot;no overlapping polygons&quot; # join each fire year, filter by area mtbs_grouped &lt;- rbind(fires_88,fires_89,fires_90,fires_91)%&gt;% mutate(area_ha = as.numeric(st_area(geometry))/10000, area_acres = area_ha*2.471) 2.2.3 Select Fires by Ecoregion and Forest Type # assign ecoregion and proportions of forest type to each fire polygon fires_join &lt;- st_join(mtbs_grouped,eco_select,join=st_intersects,left=FALSE,largest=TRUE) %&gt;% left_join(., exact_extract(conus_forestgroup,mtbs_grouped, append_cols = TRUE, max_cells_in_memory = 3e+08, fun = function(value, coverage_fraction) { data.frame(value = value, frac = coverage_fraction / sum(coverage_fraction)) %&gt;% group_by(value) %&gt;% summarize(freq = sum(frac), .groups = &#39;drop&#39;) %&gt;% pivot_wider(names_from = &#39;value&#39;, names_prefix = &#39;freq_&#39;, values_from = &#39;freq&#39;)}) %&gt;% mutate(across(starts_with(&#39;freq&#39;), replace_na, 0))) # remove unnecessary columns, cleanup names # filter to ensure fire polygons are at least 25% type of interest fires &lt;- fires_join %&gt;% dplyr::select(&quot;Fire_ID&quot;,&quot;year&quot;,&quot;area_ha&quot;,&quot;area_acres&quot;,&quot;US_L3NAME&quot;,&quot;freq_0&quot;,&quot;freq_200&quot;,&quot;freq_220&quot;,&quot;freq_260&quot;,&quot;freq_280&quot;) %&gt;% rename(&quot;ecoregion&quot; = &quot;US_L3NAME&quot;, &quot;freq_df&quot;=&quot;freq_200&quot;, &quot;freq_pp&quot;=&quot;freq_220&quot;, &quot;freq_fs&quot;=&quot;freq_260&quot;, &quot;freq_lpp&quot;=&quot;freq_280&quot;) %&gt;% mutate(freq_allother = 1-(freq_0 + freq_df+freq_pp+freq_fs+freq_lpp), freq_forested = 1- freq_0, freq_ideal = freq_df+freq_fs+freq_lpp)%&gt;% mutate(across(starts_with(&#39;freq&#39;), round,2))%&gt;% filter(freq_ideal &gt; 0.25) 2.2.4 Select Fires by Burn Severity # import all mtbs rasters via a list rastlist &lt;- list.files(path = &quot;data/mtbs&quot;, pattern=&#39;.tif&#39;, all.files=TRUE, full.names=TRUE) allrasters &lt;- lapply(rastlist, raster) names(allrasters) &lt;- str_c(&quot;y&quot;, str_sub(rastlist,22,25)) # create empty dataframe severity_list &lt;- list() # loop through mtbs mosasics for 1988-1991 # extract mtbs burn severity raster for all selected fires # calculate burn severity percentages for each fire for (i in names(allrasters)){ mtbs_year &lt;- allrasters[[i]] fire_year &lt;- filter(fires, year==str_sub(i,2,5)) raster_extract &lt;- exact_extract(mtbs_year,fire_year, max_cells_in_memory = 3e+09,coverage_area=TRUE) names(raster_extract) &lt;- fire_year$Fire_ID output_select &lt;- bind_rows(raster_extract, .id = &quot;Fire_ID&quot;)%&gt;% group_by(Fire_ID , value) %&gt;% summarize(total_area = sum(coverage_area)) %&gt;% group_by(Fire_ID) %&gt;% mutate(proportion = total_area/sum(total_area))%&gt;% dplyr::select(&quot;Fire_ID&quot;,&quot;value&quot;,&quot;proportion&quot;) %&gt;% spread(.,key=&quot;value&quot;,value = &quot;proportion&quot;) severity_list[[i]] &lt;- output_select } # combine extracted raster datasets severity_df &lt;- do.call(rbind, severity_list) # join burn severity % to fires polygons # fix naming # filter dataset for 500 acres high severity fires_severity &lt;- left_join(fires,severity_df,by=&quot;Fire_ID&quot;)%&gt;% rename(noburn= &quot;1&quot;,lowsev = &quot;2&quot;, medsev = &quot;3&quot;, highsev = &quot;4&quot;,regrowth = &quot;5&quot;, error = &quot;6&quot;) %&gt;% dplyr::select(- &quot;NaN&quot;,-&quot;regrowth&quot;,-&quot;error&quot;) %&gt;% mutate(highsev_acres = area_acres*highsev)%&gt;% filter(highsev_acres &gt; 500) 2.2.5 Clean Up Dataset # get the most common forest type within each polygon fires_select &lt;- fires_severity %&gt;% left_join(.,exact_extract(conus_forestgroup,fires_severity, &#39;mode&#39;, append_cols = TRUE, max_cells_in_memory = 3e+08)) fires_select$mode &lt;- as.factor(fires_select$mode) fires_select &lt;- fires_select %&gt;% mutate(fire_foresttype = case_when(mode==200 ~ &quot;Douglas-Fir&quot;, mode==220 ~ &quot;Ponderosa&quot;, mode==260 ~ &quot;Fir-Spruce&quot;, mode==280 ~ &quot;Lodegepole Pine&quot;, TRUE ~ &quot;Other&quot;), Fire_ID = str_c(&quot;Fire_&quot;,c(1:nrow(.)),&quot;_&quot;,year)) # join the grouped fires back to original mtbs boundaries fires_mtbs &lt;- st_join(mtbs_select,fires_select,left=FALSE,largest=TRUE) %&gt;% filter(year.x==year.y)%&gt;% dplyr::select(&quot;Event_ID&quot;,&quot;Incid_Name&quot;,&quot;Fire_ID&quot;,&quot;Ig_Date&quot;,&quot;year.y&quot;,&quot;state&quot;,&quot;BurnBndAc&quot;,&quot;ecoregion&quot;) %&gt;% rename(year= year.y) 2.3 Final Fire Dataset 2.3.1 Selected fires by year # plot mapview(fires_select, zcol = &quot;year&quot;) 2.3.2 Selected fires by majority forest type # plot mapview(fires_select, zcol = &quot;fire_foresttype&quot;) 2.4 Export Data 2.4.1 Final Cleanup for Export # reformat and project fires_export &lt;- fires_select %&gt;% mutate(year = as.integer(year)) %&gt;% st_transform(., crs=&quot;EPSG:4326&quot;) mtbs_export &lt;- fires_mtbs %&gt;% mutate(year = as.integer(year)) %&gt;% st_transform(., crs=&quot;EPSG:4326&quot;) 2.4.2 Export # st_write(fires_export, &quot;data/fire_boundaries/&quot;, &quot;fires_export.shp&quot;, driver = &#39;ESRI Shapefile&#39;) # st_write(mtbs_export, &quot;data/fire_boundaries/&quot;, &quot;mbts_export.shp&quot;, driver = &#39;ESRI Shapefile&#39;) "],["calculation-of-high-severity.html", "Part 3 Calculation of High-Severity 3.1 Set Up 3.2 Imagery 3.3 Burn Indices 3.4 Export", " Part 3 Calculation of High-Severity 3.1 Set Up 3.1.1 Import Fire Boundaries var mtbs_all = ee.FeatureCollection(&quot;USFS/GTAC/MTBS/burned_area_boundaries/v1&quot;); var fires = ee.FeatureCollection(&quot;projects/westernconiferregen/assets/fires_export&quot;); 3.1.2 Clean Fire Boundaries // filter mtbs fire perimeters to relevant date range var mtbs = mtbs_all .filter(ee.Filter.gt(&quot;Ig_Date&quot;,ee.Date(&#39;1984-01-01&#39;).millis())) .filter(ee.Filter.lt(&quot;Ig_Date&quot;,ee.Date(&#39;1992-12-31&#39;).millis())); // list fire IDs and get total number of fires var fireID = ee.List(fires.aggregate_array(&#39;Fire_ID&#39;)).getInfo(); var nFires = fireID.length; 3.2 Imagery 3.2.1 Import Landsat 5 // Landsat 5 Surface Reflectance Tier 1 collection var ls5_SR = ee.ImageCollection(&#39;LANDSAT/LT05/C01/T1_SR&#39;); 3.2.2 Prepare Landsat Data // function to get NBR, qa pixel bands var ls5_getbands = function(lsImage){ var nbr = lsImage.normalizedDifference([&#39;B4&#39;, &#39;B7&#39;]).toFloat(); var qa = lsImage.select([&#39;pixel_qa&#39;]); return nbr.addBands([qa]) .select([0,1], [&#39;nbr&#39;, &#39;pixel_qa&#39;]) .copyProperties(lsImage, [&#39;system:time_start&#39;]); }; // function to get clear pixels var ls5_qa = function(lsImg){ var quality =lsImg.select([&#39;pixel_qa&#39;]); var clear = quality.bitwiseAnd(8).eq(0) // cloud shadow .and(quality.bitwiseAnd(32).eq(0) // cloud .and(quality.bitwiseAnd(4).eq(0) // water .and(quality.bitwiseAnd(16).eq(0)))); // snow return lsImg.updateMask(clear).select([0]) .copyProperties(lsImg, [&#39;system:time_start&#39;]); }; // function to project to EPSG 4326 var ls5_project = function(lsImage){ var proj4326 = ee.Projection(&#39;EPSG:4326&#39;).atScale(30); var lsImage_proj = lsImage.reproject(proj4326); return lsImage_proj; }; // Map functions across Landsat Collection var ls5 = ls5_SR.map(ls5_getbands) .map(ls5_qa) .map(ls5_project); 3.3 Burn Indices 3.3.1 Calculate RdNBR // Calculate burn severity metrics for each fire var indices = ee.ImageCollection(fires.map(function(fire){ // get fire bounds var fireBounds = fire.geometry().bounds(); // get pre- and post-fire years var fireYear = ee.Date.parse(&#39;YYYY&#39;, fire.get(&#39;year&#39;)); var preFireYear = fireYear.advance(-1, &#39;year&#39;); var postFireYear = fireYear.advance(1, &#39;year&#39;); // filter ls5 to fire bounds and dates to get pre and post-fire imagery var preNBR = ls5.filterBounds(fireBounds) .filterDate(preFireYear, fireYear) .filter(ee.Filter.dayOfYear(152, 273)) .mean() .rename(&#39;preNBR&#39;); var postNBR = ls5.filterBounds(fireBounds) .filterDate(postFireYear, fireYear.advance(2, &#39;year&#39;)) .filter(ee.Filter.dayOfYear(152, 273)) .mean() .rename(&#39;postNBR&#39;); // calculate sqrt of pre-fire NBR to relativize var preNBRsq = preNBR .expression(&quot;abs(b(&#39;preNBR&#39;)) &lt; 0.001 ? 0.001&quot; + &quot;: b(&#39;preNBR&#39;)&quot;) .abs().sqrt().rename(&#39;preNBRsq&#39;).toFloat(); // combine pre and post-fire imagery var fireIndices = preNBR.addBands(postNBR).addBands(preNBRsq); // calculate dNBR var dnbr = fireIndices.expression(&quot;(b(&#39;preNBR&#39;) - b(&#39;postNBR&#39;)) * 1000&quot;).rename(&#39;dnbr&#39;).toFloat(); // calculate offset value from 180-m buffer of unburned area outside the fire perimeter var ring = fire.buffer(180).difference(mtbs.geometry()); var offset = ee.Image.constant(ee.Number(dnbr.select(&#39;dnbr&#39;).reduceRegion({ reducer: ee.Reducer.mean(), geometry: ring.geometry(), scale: 30, maxPixels: 1e9 }).get(&#39;dnbr&#39;))).rename(&#39;offset&#39;).toFloat().addBands(dnbr); // calculate dNBR with offset var dnbr_w_offset = fireIndices .addBands(offset.expression(&quot;b(&#39;dnbr&#39;) - b(&#39;offset&#39;)&quot;).rename(&#39;dnbr_w_offset&#39;).toFloat()); // calculate RdNBR with offset var rdnbr_w_offset = dnbr_w_offset.expression(&quot;b(&#39;dnbr_w_offset&#39;) / b(&#39;preNBRsq&#39;)&quot;).rename(&#39;rdnbr_w_offset&#39;).toFloat(); return rdnbr_w_offset.select(&#39;rdnbr_w_offset&#39;).set({&#39;fireID&#39;: fire.get(&#39;Fire_ID&#39;),&#39;fireYear&#39;: fire.get(&#39;year&#39;) }); })); 3.4 Export 3.4.1 Export Each Fire RdNBR to Drive // export to drive for (var j = 0; j &lt; nFires; j++){ var id = fireID[j]; var Name = id; var fireExport = ee.Image(indices.filterMetadata(&#39;fireID&#39;, &#39;equals&#39;, id).first()); var fireBounds = ee.Feature(fires.filterMetadata(&#39;Fire_ID&#39;, &#39;equals&#39;, id).first()).geometry().bounds(); var firePolygon = ee.Feature(fires.filterMetadata(&#39;Fire_ID&#39;, &#39;equals&#39;, id).first()).geometry(); var exportImg = fireExport.select(&#39;rdnbr_w_offset&#39;).toInt().clip(firePolygon); Export.image.toDrive({ image: exportImg, folder: &quot;fire_rdnbr_rasters&quot;, description: Name, crs: &quot;EPSG:4326&quot;, maxPixels: 1e13, scale: 30, region: fireBounds }); } "],["patch-formation.html", "Part 4 Patch Formation 4.1 Set Up 4.2 Create High-Severity Patches 4.3 Refine Patches 4.4 Mapping 4.5 Export Data", " Part 4 Patch Formation 4.1 Set Up 4.1.1 Libraries library(tidyverse) library(terra) library(patchwoRk) library(sf) library(mapview) library(exactextractr) library(lubridate) 4.1.2 Import RdNBR Rasters # import calculated RdNBR rasters for each fire boundary polygon rast_list &lt;- list.files(path = &quot;data/rdnbr_rasters&quot;, pattern=&#39;.tif&#39;, all.files=TRUE, full.names=TRUE) rast_all &lt;- lapply(rast_list, rast) rast_collection &lt;- sprc(rast_all) crs &lt;- crs(rast_collection[1]) 4.1.3 Import Fire Boundaries # import fire boundaries mtbs_export &lt;- st_read(&#39;data/fire_boundaries/mtbs_export.shp&#39;) %&gt;% st_transform(., crs=crs) fires_export &lt;- st_read(&quot;data/fire_boundaries/fires_export.shp&quot;)%&gt;% st_transform(., crs=crs) # import forest type group raster conus_forestgroup &lt;- raster(&#39;data/forest_type/conus_forestgroup.tif&#39;) forest_codes &lt;- read_csv(&#39;data/forest_type/forestgroupcodes.csv&#39;) 4.2 Create High-Severity Patches 4.2.1 PatchMorph # loop through RdNBR rasters, assign &gt;640 to high severity category # utilize patchmorph to act as 3x3 cell majority filter patch_df &lt;- list() for (i in 1:length(rast_all)){ # print(i) rast_fire &lt;- raster(rast_collection[i]) rast_fire[rast_fire &lt; 640] &lt;- 0 rast_fire[rast_fire &gt;= 640] &lt;- 1 patch &lt;- patchMorph(rast_fire, spurThresh = 3, gapThresh = 3) patch_poly &lt;- as.polygons(rast(patch)) %&gt;% st_as_sf() df_union_cast &lt;- patch_poly %&gt;% st_cast(., &quot;POLYGON&quot;) %&gt;% filter(layer == 1) patch_df[[i]] &lt;- df_union_cast} patch_poly_all &lt;- do.call(rbind,patch_df) 4.3 Refine Patches # filter small patches patches_full &lt;- patch_poly_all %&gt;% mutate(patch_area_ha = as.numeric(st_area(.))/10000) %&gt;% filter(patch_area_ha &gt; 2.25) # join patches back to grouped fires patches_joined &lt;- st_join(patches_full,mtbs_export,join = st_intersects,left= FALSE,largest = TRUE) %&gt;% dplyr::select(-layer,-BurnBndAc) %&gt;% left_join(.,exact_extract(conus_forestgroup,., &#39;mode&#39;, append_cols = TRUE, max_cells_in_memory = 3e+08))%&gt;% mutate(patch_foresttype = case_when(mode==200 ~ &quot;Douglas-Fir&quot;, mode==220 ~ &quot;Ponderosa&quot;, mode==260 ~ &quot;Fir-Spruce&quot;, mode==280 ~ &quot;Lodegepole Pine&quot;, mode==0 ~ &quot;Unforested&quot;, TRUE ~ &quot;Other&quot;)) 4.4 Mapping mapview(patches_joined,col.regions = &quot;red&quot;) + mapview(fires_export, alpha.regions = 0, lwd = 2) 4.5 Export Data patches &lt;- patches_joined %&gt;% st_transform(crs = crs) # st_write(patches, &quot;data/patches/&quot;, &quot;highsev_patches.shp&quot;,driver = &#39;ESRI Shapefile&#39;) "],["sampling-quadrants.html", "Part 5 Sampling Quadrants 5.1 Set Up 5.2 Create Sampling Quadrants 5.3 Export Data", " Part 5 Sampling Quadrants 5.1 Set Up 5.1.1 Libraries library(elevatr) library(tidyverse) library(sf) library(terra) library(mapview) 5.1.2 Import High-Severity Patches and Fire Boundaries # data import patches &lt;- st_read(&quot;data/patches/highsev_patches.shp&quot;) %&gt;% st_transform(crs=&quot;EPSG:4326&quot;) crs &lt;- crs(patches) patch_interiors&lt;- st_read(&quot;data/patches/highsev_patches_interior.shp&quot;) %&gt;% st_transform(crs=crs) patch_exteriors&lt;- st_read(&quot;data/patches/highsev_patches_exterior.shp&quot;) %&gt;% st_transform(crs=crs) mtbs_export &lt;- st_read(&#39;data/fire_boundaries/mtbs_export.shp&#39;) %&gt;% st_transform(crs=crs) fires_export &lt;- st_read(&quot;data/fire_boundaries/fires_export.shp&quot;)%&gt;% st_transform(crs=crs) 5.2 Create Sampling Quadrants 5.2.1 Split Patches by North/South Aspects and Interior/Exterior # create list of fire IDs fire_list &lt;- unique(patches$Evnt_ID) quadrants_df = list() for(i in fire_list){ # filter patch interiors/exteriors to the selected fire patch_fire &lt;- patches %&gt;% filter(Evnt_ID == i) mapview(patch_fire) patches_interior &lt;- patch_interiors %&gt;% filter(Evnt_ID == i)%&gt;% st_make_valid() %&gt;% st_union() patches_exterior &lt;- patch_exteriors %&gt;% filter(Evnt_ID_1 == i)%&gt;% st_make_valid()%&gt;% st_union() # set event and fire id to the selected fire Evnt_ID &lt;- i Fire_ID &lt;-names(which.max(table(patch_fire$Fire_ID))) print(paste0(&quot;starting event &quot;,Evnt_ID,&quot; in fire group &quot;, Fire_ID)) # get and calculate cosine corrected aspect dem &lt;- get_elev_raster(patch_fire,z=11) aspect &lt;- terrain(dem, opt = &quot;aspect&quot;,unit = &quot;radians&quot;) ccaspect &lt;- cos(aspect) # positive aspects are north-facing, negative are south-facing ccaspect[ccaspect&gt;0] &lt;- 1 ccaspect[ccaspect&lt;0] &lt;- -1 ccaspect_poly &lt;- as.polygons(rast(ccaspect)) %&gt;% st_as_sf() pos_aspect &lt;- ccaspect_poly %&gt;% filter(layer==1)%&gt;% st_make_valid() neg_aspect &lt;- ccaspect_poly %&gt;% filter(layer==-1) %&gt;% st_make_valid() # get quadrants as the intersection of interior/exterior and pos/neg aspect pos_ext &lt;- st_intersection(patches_exterior,pos_aspect)%&gt;% st_make_valid() %&gt;% st_union() %&gt;% st_as_sf()%&gt;% mutate(quadrant = &quot;pos_ext&quot;, Evnt_ID = i, quad_id_event = paste0(Evnt_ID,&quot;-&quot;,quadrant), Fire_ID = Fire_ID, quad_id_fire = paste0(Fire_ID,&quot;-&quot;,quadrant)) pos_int &lt;- st_intersection(patches_interior,pos_aspect)%&gt;% st_make_valid() %&gt;% st_union()%&gt;% st_as_sf()%&gt;% mutate(quadrant = &quot;pos_int&quot;, Evnt_ID = i, quad_id_event = paste0(Evnt_ID,&quot;-&quot;,quadrant), Fire_ID = Fire_ID, quad_id_fire = paste0(Fire_ID,&quot;-&quot;,quadrant)) neg_ext &lt;- st_intersection(patches_exterior,neg_aspect)%&gt;% st_make_valid() %&gt;% st_union() %&gt;% st_as_sf()%&gt;% mutate(quadrant = &quot;neg_ext&quot;, Evnt_ID = i, quad_id_event = paste0(Evnt_ID,&quot;-&quot;,quadrant), Fire_ID = Fire_ID, quad_id_fire = paste0(Fire_ID,&quot;-&quot;,quadrant)) neg_int &lt;- st_intersection(patches_interior, neg_aspect)%&gt;% st_make_valid() %&gt;% st_union() %&gt;% st_as_sf()%&gt;% mutate(quadrant = &quot;neg_int&quot;, Evnt_ID = i, quad_id_event = paste0(Evnt_ID,&quot;-&quot;,quadrant), Fire_ID = Fire_ID, quad_id_fire = paste0(Fire_ID,&quot;-&quot;,quadrant)) # combine, export quadrants all_quadrants &lt;- rbind(neg_int,pos_int,neg_ext,pos_ext) %&gt;% st_transform(crs=crs) quadrants_df[[i]] &lt;- all_quadrants print(paste0(&quot;completed&quot;)) } # bind list together quadrants_fullset &lt;- do.call(rbind,quadrants_df) %&gt;% st_as_sf() 5.2.2 Clean Quadrants # removes erroneous polygons created from irregular fire boundary shapes # removes small border mismatched fire quadrants_clean &lt;- quadrants_fullset %&gt;% mutate(area=as.numeric(st_area(x))) %&gt;% filter(area &gt; 1) %&gt;% group_by(Evnt_ID) %&gt;% mutate(n=n()) %&gt;% filter(n == 4) # clean up for export quadrants_export &lt;- quadrants_clean %&gt;% st_make_valid() %&gt;% st_as_sf() %&gt;% dplyr::select(-&quot;area&quot;)%&gt;% st_transform(crs=crs) 5.3 Export Data # st_write(quadrants_export,&quot;data/patches/&quot;,&quot;quadrants_export.shp&quot;,driver = &quot;ESRI Shapefile&quot;) "],["training-data.html", "Part 6 Training Data 6.1 Set Up 6.2 Import Data 6.3 Sampling Points 6.4 Export", " Part 6 Training Data 6.1 Set Up 6.1.0.1 Libraries library(tidyverse) library(sf) library(terra) 6.2 Import Data patches &lt;- st_read(&quot;data/patches/highsev_patches.shp&quot;) %&gt;% st_transform(crs=&quot;EPSG: 4326&quot;) crs &lt;- crs(patches) quadrants &lt;- st_read(&quot;data/patches/quadrants_export.shp&quot;) %&gt;% st_transform(crs=crs) 6.3 Sampling Points 6.3.1 Import and Combine Training Points points_list &lt;- list.files(path = &quot;data/points/individual_fire_points/&quot;, pattern=&#39;.shp&#39;, all.files=TRUE, full.names=TRUE) points_all &lt;- lapply(points_list, st_read) points &lt;- do.call(rbind,points_all) %&gt;% st_transform(crs=crs) 6.3.2 Assign Points and Clean Data # join points dataset back to fires to fill out dataset points_joined &lt;- st_join(points,patches,left=TRUE,largest=TRUE) %&gt;% st_join(.,quadrants,left=TRUE,largest=TRUE) ## Warning: attribute variables are assumed to be spatially constant throughout all ## geometries ## Warning: attribute variables are assumed to be spatially constant throughout all ## geometries points_cleaned &lt;- points_joined %&gt;% dplyr::select(&quot;class&quot;,&quot;ptch_r_&quot;,&quot;Evnt_ID.x&quot;,&quot;Incd_Nm&quot;,&quot;Fire_ID.x&quot;,&quot;year&quot;,&quot;ecoregn&quot;,&quot;ptch_fr&quot;,&quot;quadrnt&quot;,&quot;qd_d_vn&quot;,&quot;qd_d_fr&quot;) %&gt;% rename(patch_area_ha = ptch_r_, Event_ID = Evnt_ID.x, Incid_Name = Incd_Nm, Fire_ID = Fire_ID.x, patch_frtype = ptch_fr, quad = quadrnt, quad_event_id= qd_d_vn, quad_fire_id=qd_d_fr) %&gt;% st_transform(crs=crs) 6.4 Export # st_write(points_cleaned, &quot;data/points/&quot;, &quot;points_export.shp&quot;,driver = &#39;ESRI Shapefile&#39;) "],["snow-cover-imagery.html", "Part 7 Snow Cover Imagery 7.1 Set Up 7.2 Functions 7.3 Create Image Composites", " Part 7 Snow Cover Imagery 7.1 Set Up 7.1.1 Import Fire Boundaries // Import fire polygons var fires_export = ee.FeatureCollection(&quot;projects/westernconiferregen/assets/fires_export&quot;); 7.1.2 Import Landsat Imagery // Import Landsat 4,5,7, rename bands var ls7 = ee.ImageCollection(&#39;LANDSAT/LE07/C01/T1_SR&#39;), ls5 = ee.ImageCollection(&#39;LANDSAT/LT05/C01/T1_SR&#39;), ls4 = ee.ImageCollection(&#39;LANDSAT/LT04/C01/T1_SR&#39;); var ls4_7 = ee.ImageCollection(ls7.merge(ls5).merge(ls4)).map(function(image) { var bands = [&#39;B1&#39;,&#39;B2&#39;, &#39;B3&#39;, &#39;B4&#39;, &#39;B5&#39;, &#39;B7&#39;, &#39;pixel_qa&#39;]; var new_bands = [&#39;blue&#39;, &#39;green&#39;, &#39;red&#39;, &#39;nir&#39;, &#39;swir1&#39;, &#39;swir2&#39;, &#39;pixel_qa&#39;]; return image.select(bands).rename(new_bands); }); // Import Landsat 8, rename bands var ls8 = ee.ImageCollection(&#39;LANDSAT/LC08/C01/T1_SR&#39;).map(function(image) { var bands = [&#39;B2&#39;, &#39;B3&#39;, &#39;B4&#39;, &#39;B5&#39;, &#39;B6&#39;, &#39;B7&#39;, &#39;pixel_qa&#39;]; var new_bands = [&#39;blue&#39;, &#39;green&#39;, &#39;red&#39;, &#39;nir&#39;, &#39;swir1&#39;, &#39;swir2&#39;, &#39;pixel_qa&#39;]; return image.select(bands).rename(new_bands); }); // Merge Landsat 4-7 and 8 with renamed bands var ls4_8 = ee.ImageCollection(ls8.merge(ls4_7)); 7.2 Functions 7.2.1 Get Spectral Indices // Function: get spectral indices var calc_indices = function(image) { return image .addBands(image.normalizedDifference([&#39;nir&#39;, &#39;red&#39;]).double().rename(&#39;ndvi&#39;)) .addBands(image.normalizedDifference([&#39;green&#39;, &#39;nir&#39;]).double().rename(&#39;ndwi&#39;)) .addBands(image.normalizedDifference([&#39;nir&#39;, &#39;swir2&#39;]).double().rename(&#39;nbr&#39;)) .addBands(image.normalizedDifference([&#39;swir1&#39;,&#39;swir2&#39;]).double().rename(&#39;nbr2&#39;)) .addBands(image.normalizedDifference([&#39;green&#39;, &#39;swir1&#39;]).double().rename(&#39;ndsi&#39;)) .addBands(image.normalizedDifference([&#39;nir&#39;,&#39;swir1&#39;]).double().rename(&#39;ndfsi&#39;)) .addBands(image.expression(&#39;((NIR - R) / (NIR + R + 0.5)) * (1.5)&#39; ,{&#39;NIR&#39;:image.select(&#39;nir&#39;),&#39;R&#39;:image.select(&#39;red&#39;)}).rename(&#39;savi&#39;)) .addBands(image.expression(&#39;2.5 * ((NIR - R) / (NIR + 6 * R - 7.5 * B + 1))&#39; ,{&#39;NIR&#39;:image.select(&#39;nir&#39;),&#39;R&#39;:image.select(&#39;red&#39;),&#39;B&#39;:image.select(&#39;blue&#39;)}).rename(&#39;evi&#39;))}; 7.2.2 Get Clear Images // Function: QA var qa_mask = function(lsImg){ var quality =lsImg.select([&#39;pixel_qa&#39;]); var clear = quality.bitwiseAnd(8).eq(0) // cloud shadow .and(quality.bitwiseAnd(32).eq(0) // cloud .and(quality.bitwiseAnd(4).eq(0))); // water return lsImg.updateMask(clear) .copyProperties(lsImg, [&#39;system:time_start&#39;]); }; 7.2.3 Get snow-Covered Pixels // Function: snow masking var ndfsi_mask = function(image){ var ndfsi_snow = image.select(&#39;ndfsi&#39;).gt(0.4); return image.updateMask(ndfsi_snow); }; var ndsi_mask = function(image){ var ndsi_snow = image.select(&#39;ndsi&#39;).gt(0.4); return image.updateMask(ndsi_snow); }; 7.3 Create Image Composites 7.3.1 Map Functions Across Image Collection // Map functions to create final Landsat collection var ls_indices = ls4_8.map(calc_indices) .map(ndfsi_mask) .map(ndsi_mask) .map(qa_mask); 7.3.2 Create 3-year Landsat Composites // create 3-year snow-cover image composites over fire areas for (var j = 1986; j &lt; 1990; j++){ // list years within imagery window var years = [j-1,j,j+1,j+2]; print(&quot;landsat_&quot; + years[0] + &quot;_&quot;+ years[3]); var start = ee.Date(years[0] + &#39;-11-01&#39;); var end = ee.Date(years[3] + &#39;-05-01&#39;); print(start); print(end); // filter images to fire bounds and dates of interest var ls_area = ls_indices .filterBounds(fires_export) .filterDate(start,end) .filter(ee.Filter.calendarRange(12,4,&quot;month&quot;)) .median() .clip(fires_export); // project image to match training grid var proj4326 = ee.Projection(&#39;EPSG:4326&#39;).atScale(30); var ls_area_4326= ls_area.reproject(proj4326).clip(fires_export); // view and map print(ls_area_4326); Map.addLayer(ls_area_4326, {min:0,max:1,bands:[&quot;ndvi&quot;],palette:[&quot;white&quot;,&quot;green&quot;]}, &quot;ndvi&quot; + years[0] + &quot;_&quot; + years[3],false); Map.addLayer(ls_area_4326, {min:0,max:3000,bands:[&quot;red&quot;,&quot;green&quot;,&quot;blue&quot;]}, &quot;Landsat EPSG:4326&quot;,false); // export to drive Export.image.toDrive({ image: ls_area_4326, folder: &quot;landsat_prefire_&quot; + years[0] + &quot;_&quot; + years[3], description: &quot;landsat_prefire_&quot; + years[0] + &quot;_&quot; + years[3], crs: &quot;EPSG:4326&quot;, maxPixels: 1e13, scale: 30, region: fires_export }); } "],["model-development.html", "Part 8 Model Development 8.1 Set Up 8.2 Prepare Training Data 8.3 Examine Data 8.4 Model 8.5 Random Forest", " Part 8 Model Development 8.1 Set Up 8.1.1 Libraries library(mapview) library(sf) library(terra) library(tidyverse) library(ggplot2) library(car) library(forcats) library(randomForest) library(raster) 8.1.2 Import Snow-Cover Landsat # bring in snow-on landsat imagery tiles, merge into collection rast_list &lt;- list.files(path = &quot;data/landsat/landsat_2019_2022&quot;, pattern=&#39;.tif&#39;, all.files=TRUE, full.names=TRUE) rast_all &lt;- lapply(rast_list, rast) rast_collection &lt;- sprc(rast_all) crs &lt;- crs(rast_collection[1]) 8.1.3 Import Fire Boundaries fires_export &lt;- st_read(&quot;data/fire_boundaries/fires_export.shp&quot;)%&gt;% st_transform(., crs=crs) ## Reading layer `fires_export&#39; from data source ## `G:\\Other computers\\My Laptop\\Documents\\Grad School\\Research\\ConiferRegeneration\\data\\fire_boundaries\\fires_export.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 54 features and 20 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -118.6259 ymin: 42.57259 xmax: -106.9485 ymax: 48.9346 ## Geodetic CRS: WGS 84 8.1.4 Import Training Points # bring in training points points &lt;- st_read(&quot;data/points/points_export.shp&quot;) %&gt;% st_transform(crs=crs) ## Reading layer `points_export&#39; from data source ## `G:\\Other computers\\My Laptop\\Documents\\Grad School\\Research\\ConiferRegeneration\\data\\points\\points_export.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 3300 features and 11 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: -118.6075 ymin: 42.57725 xmax: -106.9652 ymax: 48.83974 ## Geodetic CRS: WGS 84 8.2 Prepare Training Data 8.2.1 Extract Landsat Values # extract landsat values to each training point extracted_df &lt;- list() for(i in 1:length(rast_all)){ print(i) extracted_points &lt;- st_as_sf(terra::extract(rast_collection[i], points,bind = TRUE)) extracted_df[[i]] &lt;- extracted_points } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 ## [1] 6 ## [1] 7 ## [1] 8 ## [1] 9 ## [1] 10 ## [1] 11 ## [1] 12 ## [1] 13 ## [1] 14 ## [1] 15 ## [1] 16 ## [1] 17 ## [1] 18 ## [1] 19 ## [1] 20 ## [1] 21 ## [1] 22 ## [1] 23 ## [1] 24 ## [1] 25 ## [1] 26 ## [1] 27 ## [1] 28 ## [1] 29 ## [1] 30 ## [1] 31 ## [1] 32 training_dataset &lt;- do.call(rbind,extracted_df) %&gt;% mutate(absence = as.factor(case_when(class == &quot;absence&quot; ~ &quot;absence&quot;, TRUE ~ &quot;presence&quot;)), class = fct_relevel(as.factor(class),c(&quot;absence&quot;,&quot;presencetrace&quot;,&quot;presence1to10&quot;))) %&gt;% rename(f_type = ptch_fr, area_ha = ptch_r_) %&gt;% st_drop_geometry() %&gt;% dplyr::select(-pixel_qa,-qd_vnt_,-qd_fr_d) %&gt;% drop_na(ndvi) 8.2.2 Training Data by Forest Type pres_abs_type &lt;- training_dataset %&gt;% group_by(f_type,absence) %&gt;% summarize(n=n()) %&gt;% mutate(percent= 100*n/sum(n)) %&gt;% filter(absence==&quot;presence&quot;) ## `summarise()` has grouped output by &#39;f_type&#39;. You can override using the ## `.groups` argument. pres_abs_type ## # A tibble: 7 × 4 ## # Groups: f_type [7] ## f_type absence n percent ## &lt;chr&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Douglas-Fir presence 484 75.4 ## 2 Fir-Spruce presence 516 68.3 ## 3 Lodegepole Pine presence 1064 85.7 ## 4 Other presence 25 75.8 ## 5 Ponderosa presence 19 100 ## 6 Unforested presence 399 70 ## 7 &lt;NA&gt; presence 1 50 8.2.3 Training Data by Sampling Quadrant pres_abs_quad &lt;- training_dataset %&gt;% group_by(quad,absence) %&gt;% summarize(n=n()) %&gt;% mutate(percent= 100*n/sum(n)) %&gt;% filter(absence==&quot;presence&quot;) ## `summarise()` has grouped output by &#39;quad&#39;. You can override using the ## `.groups` argument. pres_abs_quad ## # A tibble: 5 × 4 ## # Groups: quad [5] ## quad absence n percent ## &lt;chr&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 neg_ext presence 592 72.5 ## 2 neg_int presence 628 77.1 ## 3 pos_ext presence 656 79.5 ## 4 pos_int presence 631 78.5 ## 5 &lt;NA&gt; presence 1 50 8.3 Examine Data 8.3.1 Plot NDVI by Density Class ggplot(training_dataset,aes(class,ndvi)) + geom_boxplot() ggplot(training_dataset %&gt;% filter(class %in% c(&quot;absence&quot;,&quot;presencetrace&quot;)),aes(class,ndvi)) + geom_boxplot()+ ylim(-.1,.1) ## Warning: Removed 6 rows containing non-finite values (stat_boxplot). ggplot(training_dataset,aes(absence,ndvi)) + geom_boxplot() 8.3.2 Examine Variable Correlation pairs(training_dataset %&gt;% dplyr::select(&quot;class&quot;,&quot;ndvi&quot;,&quot;savi&quot;,&quot;ndsi&quot;,&quot;ndfsi&quot;,&quot;nbr2&quot;)) pairs(training_dataset %&gt;% dplyr::select(&quot;ndvi&quot;,&quot;ndfsi&quot;), pch = c(16), col = c(&quot;red&quot;,&quot;dark green&quot;)[training_dataset$absence]) cor(training_dataset %&gt;% drop_na() %&gt;% dplyr::select(ndvi, evi, savi, ndsi,ndfsi, ndwi, nbr, nbr2)) ## ndvi evi savi ndsi ndfsi ndwi ## ndvi 1.000000000 0.30491671 0.996454457 -0.7427875 -0.002754375 -0.97989942 ## evi 0.304916712 1.00000000 0.302528158 -0.2373597 0.045823412 -0.34051426 ## savi 0.996454457 0.30252816 1.000000000 -0.7409383 0.002289476 -0.98127829 ## ndsi -0.742787463 -0.23735975 -0.740938317 1.0000000 0.594820090 0.75443408 ## ndfsi -0.002754375 0.04582341 0.002289476 0.5948201 1.000000000 0.00476788 ## ndwi -0.979899418 -0.34051426 -0.981278294 0.7544341 0.004767880 1.00000000 ## nbr 0.269451200 0.17265615 0.266491644 0.1629191 0.684603934 -0.28449883 ## nbr2 0.219373399 0.13925969 0.206800607 -0.4593308 -0.314543919 -0.24868557 ## nbr nbr2 ## ndvi 0.2694512 0.2193734 ## evi 0.1726561 0.1392597 ## savi 0.2664916 0.2068006 ## ndsi 0.1629191 -0.4593308 ## ndfsi 0.6846039 -0.3145439 ## ndwi -0.2844988 -0.2486856 ## nbr 1.0000000 0.3247287 ## nbr2 0.3247287 1.0000000 8.4 Model 8.4.1 Logistic training_dataset &lt;- training_dataset %&gt;% mutate(binom=case_when(absence==&quot;absence&quot;~0, TRUE ~1), binom=as.factor(binom)) %&gt;% drop_na() # full model lm_conifer &lt;- glm(binom ~ red + green + blue + nir + swir1 + swir2 + ndsi + ndfsi + savi + ndvi + evi +nbr + nbr2 + ndwi, data = training_dataset, family = binomial(logit)) ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred vif(lm_conifer) ## red green blue nir swir1 swir2 ## 40.235624 39.136461 35.594514 19.229265 24.435774 22.559527 ## ndsi ndfsi savi ndvi evi nbr ## 11.708580 10.725242 3767.066381 3782.203445 1.459624 7.957492 ## nbr2 ndwi ## 1.679466 6.815018 # remove savi lm_conifer &lt;- glm(absence ~ red + green + blue + nir + swir1 + swir2 + ndsi + ndfsi + ndvi + evi +nbr + nbr2 + ndwi, data = training_dataset, family = binomial(logit)) ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred vif(lm_conifer) ## red green blue nir swir1 swir2 ndsi ndfsi ## 40.214274 39.126259 35.587646 19.218533 24.433118 22.555966 11.693810 10.723308 ## ndvi evi nbr nbr2 ndwi ## 5.851116 1.459246 7.955661 1.678455 6.810776 # remove red lm_conifer &lt;- glm(absence ~ green + blue + nir + swir1 + swir2 + ndsi + ndfsi + ndvi + evi +nbr + nbr2 + ndwi, data = training_dataset, family = binomial(logit)) ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred vif(lm_conifer) ## green blue nir swir1 swir2 ndsi ndfsi ndvi ## 36.396132 23.319884 17.297794 24.197945 22.315908 11.604013 10.617050 5.538773 ## evi nbr nbr2 ndwi ## 1.456638 7.927216 1.628194 6.698962 # remove swir1 lm_conifer &lt;- glm(absence ~ green + blue + nir + swir2 + ndsi + ndfsi + ndvi + evi +nbr + nbr2 + ndwi, data = training_dataset, family = binomial(logit)) ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred vif(lm_conifer) ## green blue nir swir2 ndsi ndfsi ndvi evi ## 36.049444 23.349710 16.817068 9.416591 11.644033 10.160828 5.552099 1.456431 ## nbr nbr2 ndwi ## 7.929256 1.421744 6.706886 # remove green lm_conifer &lt;- glm(absence ~ blue + nir + swir2 + ndsi + ndfsi + ndvi + evi +nbr + nbr2 + ndwi, data = training_dataset, family = binomial(logit)) ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred vif(lm_conifer) ## blue nir swir2 ndsi ndfsi ndvi evi nbr ## 6.452192 13.072599 9.336570 11.525160 10.099043 5.398357 1.456322 7.930256 ## nbr2 ndwi ## 1.419714 6.440838 # remove ndfsi lm_conifer &lt;- glm(absence ~ blue + nir + swir2 + ndsi + ndvi + evi +nbr + nbr2 + ndwi, data = training_dataset, family = binomial(logit)) ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred vif(lm_conifer) ## blue nir swir2 ndsi ndvi evi nbr nbr2 ## 6.427382 13.040423 9.307760 7.631528 5.438639 1.452517 6.103190 1.372902 ## ndwi ## 6.366622 # remove nir lm_conifer &lt;- glm(absence ~ blue + swir2 + ndsi + ndvi + evi +nbr + nbr2 + ndwi, data = training_dataset, family = binomial(logit)) ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred vif(lm_conifer) ## blue swir2 ndsi ndvi evi nbr nbr2 ndwi ## 4.316539 4.881275 7.523335 5.555723 1.456012 5.337671 1.366129 6.150173 # remove ndsi lm_conifer &lt;- glm(absence ~ blue + swir2 + ndvi + evi + nbr + nbr2 + ndwi, data = training_dataset, family = binomial(logit)) ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred vif(lm_conifer) ## blue swir2 ndvi evi nbr nbr2 ndwi ## 4.068954 4.471110 5.528265 1.454677 2.264452 1.113399 5.662548 # final model summary(lm_conifer) ## ## Call: ## glm(formula = absence ~ blue + swir2 + ndvi + evi + nbr + nbr2 + ## ndwi, family = binomial(logit), data = training_dataset) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.4229 0.0000 0.0051 0.2532 2.6226 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -9.413e+00 1.623e+00 -5.800 6.63e-09 *** ## blue 9.148e-05 4.065e-05 2.250 0.02443 * ## swir2 -2.518e-03 3.990e-04 -6.312 2.76e-10 *** ## ndvi 2.144e+01 4.604e+00 4.657 3.21e-06 *** ## evi 1.742e-01 2.301e-01 0.757 0.44899 ## nbr 1.392e+01 2.009e+00 6.926 4.34e-12 *** ## nbr2 4.097e+00 1.249e+00 3.280 0.00104 ** ## ndwi -1.895e+01 4.161e+00 -4.554 5.26e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 3525.7 on 3258 degrees of freedom ## Residual deviance: 1674.7 on 3251 degrees of freedom ## AIC: 1690.7 ## ## Number of Fisher Scoring iterations: 9 anova(lm_conifer) ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Analysis of Deviance Table ## ## Model: binomial, link: logit ## ## Response: absence ## ## Terms added sequentially (first to last) ## ## ## Df Deviance Resid. Df Resid. Dev ## NULL 3258 3525.7 ## blue 1 691.00 3257 2834.7 ## swir2 1 91.16 3256 2743.5 ## ndvi 1 958.47 3255 1785.0 ## evi 1 6.42 3254 1778.6 ## nbr 1 69.64 3253 1709.0 ## nbr2 1 13.97 3252 1695.0 ## ndwi 1 20.35 3251 1674.7 image_subset &lt;- mask(rast_collection[1],fires_export) rast_predicted &lt;- terra::predict(image_subset,lm_conifer, type=&quot;response&quot;, se.fit=TRUE) rast_predicted[rast_predicted &lt; .5] &lt;- 0 rast_predicted[rast_predicted &gt;= .5] &lt;- 1 mapview(raster(rast_predicted)) ## Warning in rasterCheckSize(x, maxpixels = maxpixels): maximum number of pixels for Raster* viewing is 5e+05 ; ## the supplied Raster* has 37748736 ## ... decreasing Raster* resolution to 5e+05 pixels ## to view full resolution set &#39;maxpixels = 37748736 &#39; 8.5 Random Forest rf_conifer &lt;- randomForest(absence ~ red + green + blue + nir + swir1 + swir2 + ndvi + savi + evi + ndsi+ndfsi+ nbr + nbr2 + ndwi, data= training_dataset) randomForest::importance(rf_conifer) ## MeanDecreaseGini ## red 50.18582 ## green 55.12659 ## blue 62.76091 ## nir 43.16774 ## swir1 51.66101 ## swir2 69.06332 ## ndvi 191.08833 ## savi 189.37411 ## evi 45.00630 ## ndsi 54.96172 ## ndfsi 60.66696 ## nbr 75.04712 ## nbr2 79.92951 ## ndwi 130.85822 summary(rf_conifer) ## Length Class Mode ## call 3 -none- call ## type 1 -none- character ## predicted 3259 factor numeric ## err.rate 1500 -none- numeric ## confusion 6 -none- numeric ## votes 6518 matrix numeric ## oob.times 3259 -none- numeric ## classes 2 -none- character ## importance 14 -none- numeric ## importanceSD 0 -none- NULL ## localImportance 0 -none- NULL ## proximity 0 -none- NULL ## ntree 1 -none- numeric ## mtry 1 -none- numeric ## forest 14 -none- list ## y 3259 factor numeric ## test 0 -none- NULL ## inbag 0 -none- NULL ## terms 3 terms call rast_predicted &lt;- terra::predict(image_subset,rf_conifer, type=&quot;response&quot;, se.fit=TRUE) mapview(raster(rast_predicted)) ## Warning in rasterCheckSize(x, maxpixels = maxpixels): maximum number of pixels for Raster* viewing is 5e+05 ; ## the supplied Raster* has 37748736 ## ... decreasing Raster* resolution to 5e+05 pixels ## to view full resolution set &#39;maxpixels = 37748736 &#39; "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
